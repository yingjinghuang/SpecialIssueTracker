#!/usr/bin/env python3
"""
Academic Journal Special Issues Scraper using Playwright
Uses real browser automation for reliable scraping
"""

import json
import os
import asyncio
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright
import re

class PlaywrightJournalScraper:
    def __init__(self):
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers',
                'backup_url': 'https://www.journals.elsevier.com/remote-sensing-of-environment/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers',
                'backup_url': 'https://www.journals.elsevier.com/cities/call-for-papers'
            }
        ]

    async def scrape_journal(self, page, journal_info: Dict) -> List[Dict]:
        """Scrape a single journal using Playwright"""
        special_issues = []
        
        try:
            print(f"ðŸ“– Scraping {journal_info['name']}...")
            
            # Try main URL first
            try:
                print(f"  â†’ Visiting {journal_info['url']}")
                await page.goto(journal_info['url'], wait_until='domcontentloaded', timeout=30000)
                await page.wait_for_timeout(2000)  # Wait for dynamic content
                
                # Get page content
                content = await page.content()
                
                # Extract special issues from the page
                issues = await self.extract_special_issues(page, journal_info['url'])
                
                if issues:
                    special_issues.extend(issues)
                    print(f"  âœ“ Found {len(issues)} special issues")
                else:
                    print(f"  âš  No issues found, trying backup URL...")
                    # Try backup URL
                    await page.goto(journal_info['backup_url'], wait_until='domcontentloaded', timeout=30000)
                    await page.wait_for_timeout(2000)
                    issues = await self.extract_special_issues(page, journal_info['backup_url'])
                    if issues:
                        special_issues.extend(issues)
                        print(f"  âœ“ Found {len(issues)} special issues from backup")
                    
            except Exception as e:
                print(f"  âœ— Error accessing page: {str(e)[:100]}")
                
        except Exception as e:
            print(f"âœ— Failed to scrape {journal_info['name']}: {str(e)[:100]}")
        
        return special_issues

    async def extract_special_issues(self, page, base_url: str) -> List[Dict]:
        """Extract special issues from the page"""
        issues = []
        
        try:
            # Method 1: Look for specific elements with special issue content
            # Try to find all headings that might be special issues
            headings = await page.query_selector_all('h2, h3, h4, h5')
            
            for heading in headings:
                try:
                    text = await heading.inner_text()
                    
                    # Skip generic headings
                    if not text or len(text) < 15:
                        continue
                    if any(skip in text.lower() for skip in ['call for papers', 'special issues', 'about', 'journal']):
                        continue
                    
                    # Get the parent container
                    parent = await heading.evaluate_handle('el => el.parentElement')
                    parent_element = parent.as_element()
                    
                    if parent_element:
                        parent_text = await parent_element.inner_text()
                        
                        # Extract information
                        issue = {
                            'title': text.strip(),
                            'deadline': self.extract_deadline(parent_text),
                            'guest_editors': self.extract_editors(parent_text),
                            'description': self.extract_description(parent_text, text),
                            'url': base_url
                        }
                        
                        # Try to find a specific link
                        link = await parent_element.query_selector('a')
                        if link:
                            href = await link.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    issue['url'] = href
                                elif href.startswith('/'):
                                    issue['url'] = 'https://www.sciencedirect.com' + href
                        
                        # Only add if we found at least a title
                        if issue['title'] and len(issue['title']) > 15:
                            issues.append(issue)
                            
                except Exception as e:
                    continue
            
            # Method 2: Look for article/section elements
            if not issues:
                sections = await page.query_selector_all('article, section, div[class*="special"], div[class*="call"]')
                
                for section in sections[:20]:  # Limit to avoid noise
                    try:
                        text = await section.inner_text()
                        
                        # Look for deadline indicators
                        if 'deadline' in text.lower() or 'submission' in text.lower():
                            heading = await section.query_selector('h2, h3, h4, h5')
                            if heading:
                                title = await heading.inner_text()
                                
                                issue = {
                                    'title': title.strip(),
                                    'deadline': self.extract_deadline(text),
                                    'guest_editors': self.extract_editors(text),
                                    'description': self.extract_description(text, title),
                                    'url': base_url
                                }
                                
                                if issue['title'] and len(issue['title']) > 15:
                                    issues.append(issue)
                    except:
                        continue
                        
        except Exception as e:
            print(f"  Error extracting issues: {str(e)[:100]}")
        
        # Deduplicate
        return self.deduplicate_issues(issues)

    def extract_deadline(self, text: str) -> str:
        """Extract deadline from text"""
        patterns = [
            r'deadline[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'due[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'submission[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'(\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*\s+\d{4})',
            r'(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+\d{4})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None

    def extract_editors(self, text: str) -> str:
        """Extract guest editors from text"""
        patterns = [
            r'guest\s+editors?[:\s]+([A-Z][^.;]+(?:[,&]\s*[A-Z][^.;]+)*)',
            r'editors?[:\s]+([A-Z][^.;]+(?:[,&]\s*[A-Z][^.;]+)*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                editors = match.group(1).strip()
                # Clean up and limit length
                if len(editors) < 300 and not any(bad in editors.lower() for bad in ['deadline', 'submission', 'manuscript']):
                    return editors
        
        return None

    def extract_description(self, text: str, title: str) -> str:
        """Extract description from text"""
        # Remove title from text
        text = text.replace(title, '')
        
        # Get first substantial sentence
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        for sentence in sentences:
            # Skip sentences with keywords that suggest it's not a description
            if any(skip in sentence.lower() for skip in ['deadline', 'submission deadline', 'guest editor', 'email', 'doi']):
                continue
            
            if len(sentence) > 60 and len(sentence) < 600:  # Reasonable description length
                return (sentence + '.').strip()
        
        return None

    def deduplicate_issues(self, issues: List[Dict]) -> List[Dict]:
        """Remove duplicate issues based on title"""
        seen = set()
        unique = []
        
        for issue in issues:
            title_key = issue['title'].lower().strip()[:50]  # Use first 50 chars
            if title_key not in seen and len(title_key) > 15:
                seen.add(title_key)
                unique.append(issue)
        
        return unique

    async def scrape_all(self) -> Dict:
        """Scrape all journals using Playwright"""
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # Launch browser in headless mode
            browser = await p.chromium.launch(headless=True)
            
            # Create a new context with reasonable settings
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            for journal_info in self.journals:
                journal_data = {
                    'name': journal_info['name'],
                    'url': journal_info['url'],
                    'special_issues': []
                }
                
                # Scrape this journal
                issues = await self.scrape_journal(page, journal_info)
                journal_data['special_issues'] = issues
                
                results['journals'].append(journal_data)
                
                # Wait a bit between journals
                await asyncio.sleep(3)
            
            await browser.close()
        
        return results

    def save_to_json(self, data: Dict, filepath: str):
        """Save scraped data to JSON file"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ’¾ Data saved to {filepath}")

async def main():
    scraper = PlaywrightJournalScraper()
    
    print("=" * 60)
    print("ðŸš€ Starting Playwright Journal Scraper")
    print("=" * 60)
    
    data = await scraper.scrape_all()
    
    # Save to data directory
    output_path = 'data/special_issues.json'
    scraper.save_to_json(data, output_path)
    
    print(f"\n{'=' * 60}")
    print(f"âœ… Scraping completed!")
    print(f"{'=' * 60}")
    print(f"Total journals: {len(data['journals'])}")
    for journal in data['journals']:
        status = "âœ“" if len(journal['special_issues']) > 0 else "âš "
        print(f"  {status} {journal['name']}: {len(journal['special_issues'])} special issues")
    print(f"{'=' * 60}\n")

if __name__ == '__main__':
    asyncio.run(main())

    
    def deduplicate_issues(self, issues: List[Dict]) -> List[Dict]:
        """Remove duplicate issues based on title"""
        seen = set()
        unique = []
        
        for issue in issues:
            title_key = issue['title'].lower().strip()
            if title_key not in seen:
                seen.add(title_key)
                unique.append(issue)
        
        return unique

    def extract_issue_info(self, element, base_url: str) -> Dict:
        """Extract information from a special issue element"""
        try:
            # Try to find title
            title_elem = element if element.name in ['h2', 'h3', 'h4'] else element.find(['h2', 'h3', 'h4'])
            if not title_elem:
                return None
            
            title = title_elem.get_text(strip=True)
            
            # Skip if title is too short or generic
            if len(title) < 10 or title.lower() in ['special issues', 'call for papers', 'about']:
                return None
            
            # Try to find related content
            parent = element.parent if element.parent else element
            content = parent.get_text(strip=True)
            
            # Extract deadline (common patterns)
            deadline = self.extract_deadline(content)
            
            # Extract guest editors
            editors = self.extract_editors(content)
            
            # Extract description
            description = self.extract_description(content, title)
            
            # Try to find URL
            link = element.find('a') or parent.find('a')
            url = link.get('href') if link else base_url
            if url and not url.startswith('http'):
                url = 'https://www.sciencedirect.com' + url
            
            return {
                'title': title,
                'deadline': deadline,
                'guest_editors': editors,
                'description': description,
                'url': url
            }
        except:
            return None

    def extract_deadline(self, text: str) -> str:
        """Extract deadline from text"""
        # Common patterns for deadlines
        patterns = [
            r'deadline[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'due[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'submission[:\s]+(\d{1,2}\s+\w+\s+\d{4})',
            r'(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None

    def extract_editors(self, text: str) -> str:
        """Extract guest editors from text"""
        # Look for editor patterns
        patterns = [
            r'guest\s+editors?[:\s]+([^.]+)',
            r'editors?[:\s]+([A-Z][^.]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                editors = match.group(1).strip()
                # Clean up and limit length
                if len(editors) < 200:
                    return editors
        
        return None

    def extract_description(self, text: str, title: str) -> str:
        """Extract description from text"""
        # Remove title from text
        text = text.replace(title, '')
        
        # Get first substantial paragraph
        sentences = text.split('.')
        description = ''
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 50:  # Substantial sentence
                description = sentence + '.'
                break
        
        # Limit length
        if len(description) > 500:
            description = description[:497] + '...'
        
        return description if description else None
    
    def get_fallback_data(self, journal_name: str) -> List[Dict]:
        """Get manually curated fallback data when scraping fails"""
        fallback_data = {
            'Remote Sensing of Environment': [
                {
                    'title': 'Remote Sensing of Ecohydrology',
                    'deadline': '31 March 2026',
                    'guest_editors': 'Hongliang Ma, Benjamin Dechant, Jiangyuan Zeng, Aleixandre Verger, William K. Smith, Youngryel Ryu',
                    'description': 'This special issue aims to highlight pioneering advancements in remote sensing of ecohydrology, fostering the exchange of leading-edge research and ideas on water-carbon coupling in terrestrial ecosystems.',
                    'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
                },
                {
                    'title': 'Large Area Monitoring with Landsat',
                    'deadline': '15 May 2026',
                    'guest_editors': 'Various Researchers',
                    'description': 'Focusing on generating essential large area datasets such as land cover, forest dynamics, phenology and evapotranspiration for addressing climate and land use challenges.',
                    'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
                }
            ],
            'Cities': [
                {
                    'title': 'The Politics of Spatial Inclusion in Cities',
                    'deadline': '28 February 2026',
                    'guest_editors': 'Rashid Mushkani, Robert J. Chaskin, Shin Koseki',
                    'description': 'This special issue examines the politics of spatial inclusion, exploring how land use, design, location, visibility, mobility, and enforcement distribute opportunities in urban spaces.',
                    'url': 'https://www.journals.elsevier.com/cities/call-for-papers'
                }
            ]
        }
        
        return fallback_data.get(journal_name, [])

    def scrape_all(self) -> Dict:
        """Scrape all configured journals"""
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        for journal_info in self.journals:
            journal_data = {
                'name': journal_info['name'],
                'url': journal_info['sources'][0]['url'] if 'sources' in journal_info else journal_info.get('url', ''),
                'special_issues': []
            }
            
            # Try scraping
            scraped_issues = self.scrape_elsevier_journal(journal_info)
            
            # Use fallback if scraping failed or returned no results
            if not scraped_issues and self.use_fallback:
                print(f"Using fallback data for {journal_info['name']}")
                scraped_issues = self.get_fallback_data(journal_info['name'])
            
            journal_data['special_issues'] = scraped_issues
            results['journals'].append(journal_data)
        
        return results

    def save_to_json(self, data: Dict, filepath: str):
        """Save scraped data to JSON file"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nData saved to {filepath}")

def main():
    scraper = JournalScraper()
    data = scraper.scrape_all()
    
    # Save to data directory
    output_path = 'data/special_issues.json'
    scraper.save_to_json(data, output_path)
    
    print(f"\n{'='*50}")
    print(f"Scraping completed!")
    print(f"{'='*50}")
    print(f"Total journals: {len(data['journals'])}")
    for journal in data['journals']:
        print(f"  âœ“ {journal['name']}: {len(journal['special_issues'])} special issues")
    print(f"{'='*50}\n")

if __name__ == '__main__':
    main()