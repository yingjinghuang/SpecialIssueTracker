#!/usr/bin/env python3
import json
import os
import asyncio
import re
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright

# å…¼å®¹æ€§å¤„ç†ï¼šé€‚é…ä¸åŒç‰ˆæœ¬çš„ playwright-stealth
try:
    from playwright_stealth import stealth_async
except ImportError:
    async def stealth_async(page):
        import playwright_stealth
        await playwright_stealth.stealth_async(page)

class PlaywrightJournalScraper:
    def __init__(self):
        # ä½¿ç”¨ä½ æä¾›æºç çš„ ScienceDirect ç›®æ ‡é¡µé¢
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        await stealth_async(page)
        
        issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            # å¢åŠ éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹ŸçœŸäºº
            await page.goto(journal_info['url'], wait_until='networkidle', timeout=90000)
            
            # æš´åŠ›ç­‰å¾…ï¼šæ— è®ºå¦‚ä½•å…ˆç­‰ 10 ç§’ï¼Œç»™ React å……åˆ†çš„æ¸²æŸ“æ—¶é—´
            await asyncio.sleep(10) 
            
            # æ¨¡æ‹Ÿä¸€ç‚¹æ»šåŠ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
            await asyncio.sleep(2)

            # è·å–å®Œæ•´çš„ HTML æºç 
            html_content = await page.content()
            print(f"   Source code obtained ({len(html_content)} chars). Scanning...")

            # æ‰§è¡Œæå–
            issues = await self.extract_logic(page, html_content)
            print(f"   âœ“ Success: Found {len(issues)} issues")

        except Exception as e:
            print(f"   âœ— Error: {str(e)[:100]}")
        finally:
            await page.close()
        return issues

    async def extract_logic(self, page, html_content: str) -> List[Dict]:
        """ç»„åˆæ‹³ï¼šDOM é€‰æ‹©å™¨ + æ­£åˆ™å…¨æ–‡æ‰«æ"""
        issues = []
        
        # 1. é¦–å…ˆå°è¯•æœ€æ­£è§„çš„ DOM æå–
        items = await page.query_selector_all('li.list-item')
        for item in items:
            try:
                title_link = await item.query_selector('a[href*="/special-issue/"]')
                if title_link:
                    title = await title_link.inner_text()
                    href = await title_link.get_attribute('href')
                    issues.append({
                        'title': title.strip(),
                        'url': 'https://www.sciencedirect.com' + href if href.startswith('/') else href,
                        'deadline': "Parsing...",
                        'last_updated': datetime.now().strftime('%Y-%m-%d')
                    })
            except: continue

        # 2. å¦‚æœ DOM æå–å¤±è´¥ï¼Œå¯åŠ¨æ­£åˆ™æ‰«æ (æš´åŠ›æå–æ‰€æœ‰ SI é“¾æ¥)
        if not issues:
            # åŒ¹é…æ¨¡å¼ï¼šå¯»æ‰¾ /special-issue/ å¼€å¤´çš„é“¾æ¥åŠå…¶å‰åçš„æ–‡æœ¬
            # è¿™ä¸ªæ­£åˆ™ä¼šæŠ“å– href åŠå…¶æ ‡ç­¾å†…çš„æ–‡æœ¬
            pattern = r'href="(/special-issue/[^"]+)"[^>]*>.*?<span>(.*?)</span>'
            matches = re.findall(pattern, html_content, re.DOTALL)
            
            for href, title in matches:
                # è¿‡æ»¤æ‰ HTML æ ‡ç­¾
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                if len(clean_title) > 10:
                    issues.append({
                        'title': clean_title,
                        'url': 'https://www.sciencedirect.com' + href,
                        'deadline': "Check website",
                        'last_updated': datetime.now().strftime('%Y-%m-%d')
                    })

        return self.deduplicate(issues)

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            key = i['title'].lower().strip()
            if key not in seen:
                seen.add(key)
                unique.append(i)
        return unique

    async def run(self):
        print("=" * 60)
        print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # å¿…é¡»ä½¿ç”¨ chromium å¹¶åœ¨ headless æ¨¡å¼ä¸‹é…ç½®çœŸå®çš„ä¸Šä¸‹æ–‡
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'url': journal['url'],
                    'special_issues': issues
                })
                # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé˜²æ­¢ IP è§¦å‘äºŒæ¬¡æ‹¦æˆª
                await asyncio.sleep(5)

            await browser.close()
            
        # ä¿å­˜ç»“æœ
        os.makedirs('data', exist_ok=True)
        with open('data/special_issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Scraping completed. Data saved to data/special_issues.json")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())