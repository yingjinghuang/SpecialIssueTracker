#!/usr/bin/env python3
import json
import os
import asyncio
import re
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright

# å…¼å®¹æ€§å¤„ç†ï¼šé€‚é…ä¸åŒç‰ˆæœ¬çš„ playwright-stealth
try:
    from playwright_stealth import stealth_async
except ImportError:
    async def stealth_async(page):
        import playwright_stealth
        await playwright_stealth.stealth_async(page)

class PlaywrightJournalScraper:
    def __init__(self):
        # ä½¿ç”¨ä½ æä¾›æºç çš„ ScienceDirect ç›®æ ‡é¡µé¢
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        await stealth_async(page)
        
        issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            # å¢åŠ éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹ŸçœŸäºº
            await page.goto(journal_info['url'], wait_until='networkidle', timeout=90000)
            
            # æš´åŠ›ç­‰å¾…ï¼šæ— è®ºå¦‚ä½•å…ˆç­‰ 10 ç§’ï¼Œç»™ React å……åˆ†çš„æ¸²æŸ“æ—¶é—´
            await asyncio.sleep(10) 
            
            # æ¨¡æ‹Ÿä¸€ç‚¹æ»šåŠ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
            await asyncio.sleep(2)

            # è·å–å®Œæ•´çš„ HTML æºç 
            html_content = await page.content()
            print(f"   Source code obtained ({len(html_content)} chars). Scanning...")

            # æ‰§è¡Œæå–
            issues = await self.extract_logic(page, html_content)
            print(f"   âœ“ Success: Found {len(issues)} issues")

        except Exception as e:
            print(f"   âœ— Error: {str(e)[:100]}")
        finally:
            await page.close()
        return issues

    async def extract_logic(self, page, html_content: str) -> List[Dict]:
        """ä»éšè—çš„ JSON å˜é‡æˆ–åŸå§‹æ–‡æœ¬ä¸­æŒ–æ˜æ•°æ®"""
        issues = []
        
        print("   ğŸ” Deep scanning source for hidden data patterns...")

        # æ–¹æ¡ˆ A: å¯»æ‰¾é“¾æ¥æ¨¡å¼ (ä¸å¸¦ HTML æ ‡ç­¾ï¼Œç›´æ¥æœå­—ç¬¦ä¸²)
        # åŒ¹é… URL: /special-issue/æ•°å­—/æ ‡é¢˜
        links = re.findall(r'/special-issue/(\d+)/([^"\' >]+)', html_content)
        for issue_id, slug in links:
            # å°† slug è½¬æ¢ä¸ºå¯è¯»æ ‡é¢˜ (ä¾‹å¦‚ geospatial-foundation-models -> Geospatial Foundation Models)
            title = slug.replace('-', ' ').title()
            issues.append({
                'title': title,
                'url': f'https://www.sciencedirect.com/special-issue/{issue_id}/{slug}',
                'deadline': "Check Link",
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })

        # æ–¹æ¡ˆ B: å¯»æ‰¾ JSON æ•°ç»„ (ScienceDirect å¸¸è§çš„å†…éƒ¨å­˜å‚¨æ ¼å¼)
        # å¯»æ‰¾åŒ…å« "specialIssueTitle" æˆ– "submissionDeadline" çš„ JSON å—
        json_blobs = re.findall(r'\{"title":"[^"]+","url":"[^"]*special-issue[^"]*"\}', html_content)
        for blob in json_blobs:
            try:
                data = json.loads(blob)
                issues.append({
                    'title': data.get('title', 'Unknown'),
                    'url': 'https://www.sciencedirect.com' + data.get('url', ''),
                    'deadline': data.get('deadline', 'Unknown'),
                    'last_updated': datetime.now().strftime('%Y-%m-%d')
                })
            except: continue

        # æ–¹æ¡ˆ C: é’ˆå¯¹ä½ æä¾›çš„æºç ä¸­å‡ºç°çš„å…·ä½“æ–‡æ¡ˆè¿›è¡Œæ­£åˆ™å®šä½
        # å¯»æ‰¾ <h3><span>...</span></h3> è¿™ç§ç‰¹å®šç»“æ„
        matches = re.findall(r'<span>([^<]{15,100}?)</span>', html_content)
        for match in matches:
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„éæ ‡é¢˜æ–‡æ¡ˆ
            if any(x in match.lower() for x in ['cookie', 'elsevier', 'sciencedirect', 'rights reserved']):
                continue
            # å¦‚æœçœ‹èµ·æ¥åƒä¸ªå­¦æœ¯æ ‡é¢˜ï¼Œå°±æ”¶å½•
            issues.append({
                'title': match.strip(),
                'url': "Search on site",
                'deadline': "Unknown",
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })

        # å»é‡å¹¶è¿‡æ»¤æ‰åƒåœ¾ä¿¡æ¯
        return self.deduplicate(issues)

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            key = i['title'].lower().strip()
            if key not in seen:
                seen.add(key)
                unique.append(i)
        return unique

    async def run(self):
        print("=" * 60)
        print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # å¿…é¡»ä½¿ç”¨ chromium å¹¶åœ¨ headless æ¨¡å¼ä¸‹é…ç½®çœŸå®çš„ä¸Šä¸‹æ–‡
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'url': journal['url'],
                    'special_issues': issues
                })
                # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé˜²æ­¢ IP è§¦å‘äºŒæ¬¡æ‹¦æˆª
                await asyncio.sleep(5)

            await browser.close()
            
        # ä¿å­˜ç»“æœ
        os.makedirs('data', exist_ok=True)
        with open('data/special_issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Scraping completed. Data saved to data/special_issues.json")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())