import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥
API_KEY = os.environ.get('SCRAPER_API_KEY')

# å®šä¹‰ç›®æ ‡æœŸåˆŠ
JOURNALS = [
    {
        'name': 'Remote Sensing of Environment',
        'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
    },
    {
        'name': 'Cities',
        'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
    }
]

def get_soup(target_url):
    """
    é€šè¿‡ ScraperAPI è·å–æ¸²æŸ“åçš„ HTML
    """
    if not API_KEY:
        raise ValueError("âŒ ç¼ºå°‘ API Keyï¼è¯·åœ¨ GitHub Secrets ä¸­é…ç½® SCRAPER_API_KEY")

    payload = {
        'api_key': API_KEY,
        'url': target_url,
        'render': 'true',  # å…³é”®ï¼šå‘Šè¯‰ API å¸®æˆ‘ä»¬æ¸²æŸ“ JS
        # 'country_code': 'us', # å¯é€‰ï¼šæŒ‡å®šç¾å›½ IP
    }
    
    print(f"   â˜ï¸ Calling ScraperAPI for: {target_url} ...")
    try:
        r = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
        if r.status_code == 200:
            print("   âœ… Success! Content received.")
            return BeautifulSoup(r.text, 'html.parser')
        else:
            print(f"   âŒ Failed: {r.status_code} - {r.text}")
            return None
    except Exception as e:
        print(f"   âŒ Error: {e}")
        return None

def parse_journal(journal):
    print(f"ğŸ“– Scraping {journal['name']}...")
    soup = get_soup(journal['url'])
    issues = []
    
    if not soup:
        return []

    # BeautifulSoup æŸ¥æ‰¾é€»è¾‘
    # å¯»æ‰¾æ‰€æœ‰ href åŒ…å« special-issue çš„ a æ ‡ç­¾
    links = soup.select('a[href*="/special-issue/"]')
    print(f"   ğŸ” Found {len(links)} raw links.")

    seen_urls = set()
    
    for link in links:
        title = link.get_text(strip=True)
        url = link.get('href')
        
        if not title or not url:
            continue
            
        # è¡¥å…¨ URL
        if not url.startswith('http'):
            url = 'https://www.sciencedirect.com' + url
            
        if url not in seen_urls:
            seen_urls.add(url)
            issues.append({
                'title': title,
                'url': url,
                'deadline': 'Check Link', # å¦‚æœæƒ³è¿›ä¸€æ­¥æŠ“è¯¦æƒ…ï¼Œéœ€è¦å†è°ƒä¸€æ¬¡ API
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })
            
    print(f"   âœ… Extracted {len(issues)} unique issues.")
    return issues

def main():
    print("=" * 60)
    print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'journals': []
    }
    
    for journal in JOURNALS:
        issues = parse_journal(journal)
        results['journals'].append({
            'name': journal['name'],
            'url': journal['url'],
            'special_issues': issues
        })
    
    # ä¿å­˜ç»“æœ
    os.makedirs('data', exist_ok=True)
    with open('data/issues.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ’¾ Data saved to data/issues.json")
    print("=" * 60)

if __name__ == "__main__":
    main()