import os
import json
import requests
import re
import time
from bs4 import BeautifulSoup
from datetime import datetime

# ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥
API_KEY = os.environ.get('SCRAPER_API_KEY')

def load_journals():
    try:
        with open('journals.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âŒ Error: journals.json not found.")
        return []

def get_soup(target_url):
    if not API_KEY:
        print("âŒ ç¼ºå°‘ API Keyï¼")
        return None

    payload = {
        'api_key': API_KEY,
        'url': target_url,
        'render': 'true', 
    }
    
    try:
        # é‡è¯• 3 æ¬¡
        for attempt in range(3):
            r = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
            if r.status_code == 200:
                return BeautifulSoup(r.text, 'html.parser')
            print(f"   âš ï¸ Attempt {attempt+1} failed: {r.status_code}. Retrying...")
            time.sleep(2)
        return None
    except Exception as e:
        print(f"   âŒ Network Error: {e}")
        return None

def extract_details(soup):
    """
    ä»è¯¦æƒ…é¡µæå–ï¼šæˆªæ­¢æ—¥æœŸã€è´£ä»»ç¼–è¾‘ã€ç®€ä»‹
    """
    if not soup:
        return {"deadline": "Unknown", "editors": "Unknown", "description": ""}

    text_content = soup.get_text(" ", strip=True)
    
    # --- 1. æå–æˆªæ­¢æ—¥æœŸ ---
    deadline = "Check Detail"
    date_patterns = [
        r'Submission deadline:?\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})',
        r'deadline for manuscript submissions is\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})',
        r'submission deadline is\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})'
    ]
    for pattern in date_patterns:
        match = re.search(pattern, text_content, re.IGNORECASE)
        if match:
            deadline = match.group(1)
            break
            
    # --- 2. æå–è´£ä»»ç¼–è¾‘ (Guest Editors) ---
    editors = "Unknown"
    # ScienceDirect çš„ç¼–è¾‘é€šå¸¸åœ¨ "Guest editors" æ ‡é¢˜ä¸‹
    # æˆ‘ä»¬å°è¯•æ‰¾åŒ…å« "Guest editors" çš„å…ƒç´ ï¼Œç„¶åæ‰¾å®ƒçš„å…„å¼ŸèŠ‚ç‚¹æˆ–å­èŠ‚ç‚¹
    try:
        # æ–¹æ³• A: ç®€å•çš„æ–‡æœ¬æŸ¥æ‰¾æˆªå– (æ¯”è¾ƒæš´åŠ›ä½†æœ‰æ•ˆ)
        editor_match = re.search(r'Guest editors?\s*:?\s*(.*?)(?=\s*(Submission|Manuscript|Inquiries|$))', text_content, re.IGNORECASE)
        if editor_match:
            editors_raw = editor_match.group(1).strip()
            # æˆªå–å‰ 100 ä¸ªå­—ç¬¦ï¼Œé˜²æ­¢æŠ“åˆ°æ— å…³æ–‡æœ¬
            editors = editors_raw[:100] + "..." if len(editors_raw) > 100 else editors_raw
    except:
        pass

    # --- 3. æå–ç®€ä»‹/è¯¦ç»†ä»‹ç» ---
    description = ""
    try:
        # å°è¯•å¯»æ‰¾æ­£æ–‡åŒºåŸŸï¼ŒScienceDirect ç»“æ„å¤šå˜ï¼Œè¿™é‡Œå–ç½‘é¡µä¸»è¦æ–‡æœ¬
        # ç§»é™¤ script å’Œ style
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # è·å–æ¸…æ´—åçš„æ–‡æœ¬
        clean_text = soup.get_text(" ", strip=True)
        # ç®€å•æ¸…æ´—ï¼šæ‰¾åˆ° "Call for papers" æˆ–æ ‡é¢˜ä¹‹åçš„å†…å®¹
        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„åˆ‡ç‰‡ï¼Œä¿ç•™å‰ 500 ä¸ªå­—ç¬¦ä½œä¸ºç®€ä»‹
        description = clean_text[:500] + "..."
    except:
        description = "No description extracted."

    return {
        "deadline": deadline,
        "editors": editors,
        "description": description
    }

def parse_journal(journal):
    print(f"ğŸ“– Scanning List: {journal['name']}...")
    soup = get_soup(journal['url'])
    issues = []
    
    if not soup: return []

    links = soup.select('a[href*="/special-issue/"]')
    print(f"   ğŸ” Found {len(links)} issues in list.")

    seen_urls = set()
    
    # âš ï¸ æ³¨æ„ï¼šä¸ºäº†æµ‹è¯•ï¼Œæˆ‘è¿™é‡Œè¿˜æ˜¯é™åˆ¶æŠ“å–å‰ 5 ä¸ª
    # å¦‚æœè¦å…¨æŠ“ï¼Œè¯·å»æ‰ [:5]
    for link in links[:5]: 
        title = link.get_text(strip=True)
        url = link.get('href')
        
        if not title or not url: continue
        if not url.startswith('http'): url = 'https://www.sciencedirect.com' + url
            
        if url not in seen_urls:
            seen_urls.add(url)
            print(f"      â˜ï¸ Deep diving: {title[:30]}...")
            
            # è¿›å…¥è¯¦æƒ…é¡µ
            detail_soup = get_soup(url)
            
            # æå–æ‰€æœ‰è¯¦æƒ…
            details = extract_details(detail_soup)
            
            print(f"      ğŸ—“ï¸ Deadline: {details['deadline']}")
            print(f"      ğŸ‘¥ Editors: {details['editors'][:30]}...")
            
            issues.append({
                'title': title,
                'url': url,
                'deadline': details['deadline'],
                'guest_editors': details['editors'],   # æ–°å¢å­—æ®µ
                'description': details['description'], # æ–°å¢å­—æ®µ
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            })
            
    return issues

def main():
    print("=" * 60)
    print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    journals = load_journals()
    if not journals: return

    results = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'journals': []
    }
    
    for journal in journals:
        issues = parse_journal(journal)
        results['journals'].append({
            'name': journal['name'],
            'url': journal['url'],
            'special_issues': issues
        })
    
    os.makedirs('data', exist_ok=True)
    with open('data/issues.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ’¾ Data saved to data/issues.json")
    print("=" * 60)

if __name__ == "__main__":
    main()