#!/usr/bin/env python3
import json
import os
import asyncio
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright

# å°è¯•å¯¼å…¥ stealthï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿä¸å¼ºæ±‚ï¼ˆGithub Actions é‡Œå¯èƒ½éœ€è¦ç‰¹æ®Šé…ç½®ï¼‰
try:
    from playwright_stealth import stealth_async
except ImportError:
    async def stealth_async(page): pass

class PlaywrightJournalScraper:
    def __init__(self):
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        await stealth_async(page)
        
        issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            # å¢åŠ  timeout é˜²æ­¢ç½‘ç»œæ…¢æŠ¥é”™
            await page.goto(journal_info['url'], wait_until='domcontentloaded', timeout=60000)
            
            # ç­‰å¾… 5 ç§’è®© JS åŠ è½½
            await asyncio.sleep(5) 
            
            # --- å…³é”®è°ƒè¯•æ­¥éª¤ ---
            # å¦‚æœæŠ“ä¸åˆ°æ•°æ®ï¼ŒæŸ¥çœ‹å½“å‰ç›®å½•ä¸‹ç”Ÿæˆçš„ debug_æˆªå›¾.pngï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯å‡ºç°äº†éªŒè¯ç 
            screenshot_path = f"debug_{journal_info['name'].replace(' ', '_')}.png"
            await page.screenshot(path=screenshot_path)
            print(f"   ğŸ“¸ Debug screenshot saved to {screenshot_path}")
            # --------------------

            # ä½¿ç”¨ Locator æŸ¥æ‰¾æ‰€æœ‰åŒ…å« special-issue çš„é“¾æ¥
            # ScienceDirect çš„ç»“æ„é€šå¸¸æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬ç›´æ¥æ‰¾ href é‡Œå¸¦ special-issue çš„ a æ ‡ç­¾
            links = page.locator('a[href*="/special-issue/"]')
            
            count = await links.count()
            print(f"   Found {count} potential links via Locator.")

            for i in range(count):
                element = links.nth(i)
                title = await element.text_content()
                url = await element.get_attribute('href')
                
                # ç®€å•æ¸…æ´—æ•°æ®
                if title and url:
                    # è¡¥å…¨ URL
                    full_url = url if url.startswith('http') else f"https://www.sciencedirect.com{url}"
                    
                    issues.append({
                        'title': title.strip(),
                        'url': full_url,
                        'deadline': 'Check Link', # deadline å¾€å¾€è—åœ¨è¯¦æƒ…é¡µï¼Œåˆ—è¡¨é¡µå¾ˆéš¾æŠ“å‡†ï¼Œå…ˆç•¥è¿‡
                        'last_updated': datetime.now().strftime('%Y-%m-%d')
                    })

        except Exception as e:
            print(f"   âœ— Error: {e}")
        finally:
            await page.close()
        
        return self.deduplicate(issues)

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            # ç”¨ URL åšå»é‡é”®æ›´å‡†ç¡®
            key = i['url']
            if key not in seen:
                seen.add(key)
                unique.append(i)
        return unique

    async def run(self):
        print("=" * 60)
        print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # âš ï¸ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ é˜²æ£€æµ‹å‚æ•°
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled', # ç§»é™¤è‡ªåŠ¨åŒ–ç‰¹å¾
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-infobars',
                    '--window-position=0,0',
                    '--ignore-certificate-errors',
                    '--window-size=1920,1080',
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36' 
                ]
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                # å†æ¬¡è¦†ç›– User Agent ç¡®ä¿ä¸‡æ— ä¸€å¤±
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
            )
            
            # ä¸ºæ¯ä¸ªé¡µé¢æ³¨å…¥ JSï¼Œå½»åº•æŠ¹é™¤ webdriver ç—•è¿¹
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'url': journal['url'],
                    'special_issues': issues
                })
                # æ‰“å°ä¸€ä¸‹ç»“æœé¢„è§ˆ
                print(f"   âœ… Collected {len(issues)} issues.")
                await asyncio.sleep(3) # ä¼‘æ¯ä¸€ä¸‹

            await browser.close()
            
        # ä¿å­˜ç»“æœ
        os.makedirs('data', exist_ok=True)
        with open('data/issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Data saved to data/issues.json")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())