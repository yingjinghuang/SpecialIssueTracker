#!/usr/bin/env python3
import json
import os
import asyncio
import random
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright

# å°è¯•å¯¼å…¥ stealth
try:
    from playwright_stealth import stealth_async
except ImportError:
    async def stealth_async(page): pass

class PlaywrightJournalScraper:
    def __init__(self):
        self.journals = [
            {
                'name': 'Remote Sensing of Environment',
                'url': 'https://www.sciencedirect.com/journal/remote-sensing-of-environment/about/call-for-papers'
            },
            {
                'name': 'Cities',
                'url': 'https://www.sciencedirect.com/journal/cities/about/call-for-papers'
            }
        ]

    async def scrape_journal(self, context, journal_info: Dict) -> List[Dict]:
        page = await context.new_page()
        await stealth_async(page)
        
        issues = []
        try:
            print(f"ğŸ“– Scraping {journal_info['name']}...")
            
            # --- å…³é”®ç­–ç•¥ 1: ä¼ªè£… Referer (å‡è£…æ¥è‡ª Google) ---
            await page.set_extra_http_headers({
                "Referer": "https://www.google.com/",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            })

            # --- å…³é”®ç­–ç•¥ 2: è¿‚å›æˆ˜æœ¯ (å…ˆè®¿é—®é¦–é¡µé¢† Cookie) ---
            print("   Drafting cookies from homepage...")
            try:
                await page.goto("https://www.sciencedirect.com/", wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(random.uniform(2, 4)) # å‡è£…äººåœ¨çœ‹é¦–é¡µ
            except Exception as e:
                print(f"   âš ï¸ Homepage load warning: {e}")

            # --- å…³é”®ç­–ç•¥ 3: è·³è½¬åˆ°ç›®æ ‡é¡µ ---
            print("   Navigating to target page...")
            response = await page.goto(journal_info['url'], wait_until='domcontentloaded', timeout=60000)
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            page_content = await page.content()
            if "There was a problem providing the content" in page_content or response.status == 403:
                print(f"   ğŸš« Blocked! Taking screenshot...")
                await page.screenshot(path=f"blocked_{journal_info['name'].replace(' ', '_')}.png")
                return []

            # æ­£å¸¸ç­‰å¾…æ¸²æŸ“
            await asyncio.sleep(5) 
            
            # æˆªå›¾ç•™è¯ (æ— è®ºæˆåŠŸå¤±è´¥éƒ½å­˜ä¸€å¼ ï¼Œæ–¹ä¾¿è°ƒè¯•)
            await page.screenshot(path=f"debug_{journal_info['name'].replace(' ', '_')}.png")

            # æŸ¥æ‰¾é“¾æ¥ (é’ˆå¯¹ ScienceDirect çš„ç»“æ„è°ƒæ•´)
            # å¯»æ‰¾ href ä¸­åŒ…å« /special-issue/ çš„é“¾æ¥
            links = page.locator('a[href*="/special-issue/"]')
            count = await links.count()
            print(f"   Found {count} potential links.")

            for i in range(count):
                element = links.nth(i)
                title = await element.text_content()
                url = await element.get_attribute('href')
                
                if title and url:
                    full_url = url if url.startswith('http') else f"https://www.sciencedirect.com{url}"
                    issues.append({
                        'title': title.strip(),
                        'url': full_url,
                        'deadline': 'Check Link',
                        'last_updated': datetime.now().strftime('%Y-%m-%d')
                    })

        except Exception as e:
            print(f"   âœ— Error: {e}")
            await page.screenshot(path=f"error_{journal_info['name'].replace(' ', '_')}.png")
        finally:
            await page.close()
        
        return self.deduplicate(issues)

    def deduplicate(self, issues: List[Dict]) -> List[Dict]:
        seen = set()
        unique = []
        for i in issues:
            key = i['url']
            if key not in seen:
                seen.add(key)
                unique.append(i)
        return unique

    async def run(self):
        print("=" * 60)
        print(f"ğŸš€ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'journals': []
        }
        
        async with async_playwright() as p:
            # ä½¿ç”¨ç¨å¾®æ—§ä¸€ç‚¹çš„ User-Agentï¼Œæœ‰æ—¶å€™åè€Œæ›´ç¨³
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            context = await browser.new_context(
                viewport={'width': 1366, 'height': 768}, # æ™®é€šç¬”è®°æœ¬åˆ†è¾¨ç‡
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
            )
            
            # æ³¨å…¥webdriverç§»é™¤è„šæœ¬
            await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            for journal in self.journals:
                issues = await self.scrape_journal(context, journal)
                results['journals'].append({
                    'name': journal['name'],
                    'url': journal['url'],
                    'special_issues': issues
                })
                print(f"   âœ… Collected {len(issues)} issues.")
                # å¿…é¡»ä¼‘æ¯ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å° IP
                await asyncio.sleep(random.uniform(5, 10))

            await browser.close()
            
        os.makedirs('data', exist_ok=True)
        with open('data/issues.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Data saved to data/issues.json")
        print("=" * 60)

if __name__ == "__main__":
    asyncio.run(PlaywrightJournalScraper().run())